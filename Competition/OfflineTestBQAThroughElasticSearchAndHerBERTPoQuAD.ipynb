{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f907e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from elasticsearch.helpers import streaming_bulk\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModel\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance\n",
    "import torch\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62f505",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60375a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/test_B_questions.txt', 'r', encoding='UTF-8') as f:\n",
    "    questions = []\n",
    "    for line in f:\n",
    "        questions.append(line.strip())\n",
    "        \n",
    "with open('../Data/test_B_answers.txt', 'r', encoding='UTF-8') as f:\n",
    "    answers = []\n",
    "    for line in f:\n",
    "        splitted = line.strip().split(\"\\t\")\n",
    "        answers.append(splitted)\n",
    "\n",
    "question_answers = list(zip(questions, answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0fc7653",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [qa[0] for qa in question_answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc5f65",
   "metadata": {},
   "source": [
    "# Pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f88c5987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jakub\\anaconda3\\envs\\PytorchCPU\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1132: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "yes_no_model = AutoModelWithLMHead.from_pretrained('flax-community/papuGaPT2')\n",
    "yes_no_tokenizer = AutoTokenizer.from_pretrained('flax-community/papuGaPT2')\n",
    "# yes_no_tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-large-cased\")\n",
    "# yes_no_model = AutoModelWithLMHead.from_pretrained(\"allegro/herbert-large-cased\")\n",
    "_ = yes_no_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfca6322",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b265af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline_herbert = pipeline(\n",
    "    \"question-answering\",\n",
    "    model='azwierzc/herbert-large-poquad', handle_impossible_answer=True,\n",
    "    tokenizer='azwierzc/herbert-large-poquad'\n",
    ")\n",
    "\n",
    "def get_answer_herbert(context, question):\n",
    "    return qa_pipeline_herbert({\n",
    "        'context': context,\n",
    "        'question': question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec335267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PLT5 Large is about 3x slower than PLT5 Base\n",
    "# plt5_tokenizer = AutoTokenizer.from_pretrained(\"azwierzc/plt5-large-poquad\")\n",
    "# plt5_model = T5ForConditionalGeneration.from_pretrained(\"azwierzc/plt5-large-poquad\")\n",
    "# _ = plt5_model.eval()\n",
    "plt5_tokenizer = AutoTokenizer.from_pretrained(\"azwierzc/plt5-base-poquad\")\n",
    "plt5_model = T5ForConditionalGeneration.from_pretrained(\"azwierzc/plt5-base-poquad\")\n",
    "_ = plt5_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1d3e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_plt5(context, question):\n",
    "    query = plt5_tokenizer(f\"question: {question}, context: {context}\", \n",
    "                           max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    output = plt5_model.generate(**query, max_new_tokens=8, num_beams=5, return_dict_in_generate=True, output_scores=True)\n",
    "    decoded_output = plt5_tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "    score = np.exp(output.sequences_scores.numpy()[0])\n",
    "    return {'score': score, 'answer': decoded_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ee40b",
   "metadata": {},
   "source": [
    "# Connect to elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb961616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Password for the 'elastic' user generated by Elasticsearch\n",
    "ELASTIC_PASSWORD = YOUR_ELASTICSEARCH_PASSWORD\n",
    "# es_path = \"~/Documents/UWr/Chatbots/elasticsearch-8.4.3/\"\n",
    "es_path = \"C:/Users/jakub/elasticsearch-8.5.3-windows-x86_64/elasticsearch-8.5.3/\"\n",
    "\n",
    "# Create the client instance\n",
    "client = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    ca_certs=es_path+\"config/certs/http_ca.crt\",\n",
    "    basic_auth=(\"elastic\", ELASTIC_PASSWORD)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b715b014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if you can connect to ES (bool)\n",
    "client.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e81d04",
   "metadata": {},
   "source": [
    "# Index documents from wikipedia paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48f87869",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILE = \"../Data/fp_wiki.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20d37483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_actions():\n",
    "    with open(DATASET_FILE, \"r\", encoding=\"UTF-8\") as file:\n",
    "        # Read the first line of the file\n",
    "        title_line = file.readline()\n",
    "        # Create a variable to store the ID of the next document\n",
    "        next_id = 0\n",
    "        # Keep reading lines until the end of the file is reached\n",
    "        while title_line:\n",
    "            # Check if the line starts with \"Title: \"\n",
    "            if title_line.startswith(\"TITLE: \"):\n",
    "                # Get the title by stripping the \"Title: \" prefix and the newline character at the end\n",
    "                title = title_line.lstrip(\"TITLE:\").strip()\n",
    "                # Read the second line of the file, which should be the title again\n",
    "                title_line = file.readline().strip()\n",
    "                # Save title for later usage\n",
    "                title = title_line\n",
    "                # Create a list to store the lines of the article\n",
    "                article_lines = []\n",
    "                # Read the next line, which should be the start of the article\n",
    "                article_line = file.readline()\n",
    "                # Keep reading lines until an empty line is reached\n",
    "                while article_line.strip():\n",
    "                    # Add the line to the list of article lines\n",
    "                    article_lines.append(article_line)\n",
    "                    # Read the next line\n",
    "                    article_line = file.readline()\n",
    "                # Join the lines of the article with newline characters to create the article\n",
    "                article = \"\\n\".join(article_lines) if article_lines else \"\"\n",
    "                # Create a dictionary for the document\n",
    "                document = {\"_id\": next_id, \"title\": title, \"article\": article}\n",
    "                # Yield new document\n",
    "                yield document\n",
    "                # Increment the ID for the next document\n",
    "                next_id += 1\n",
    "                # Read the next line, which should be the start of the next document\n",
    "                title_line = file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c5eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"offline_competition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84f2ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"lang_pl_morfologik\": { \n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"morfologik_stem\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"long\"},\n",
    "            \"article\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"lang_pl_morfologik\"\n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"lang_pl_morfologik\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff863ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'offline_competition'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.options(ignore_status=[400,404]).indices.delete(index=index_name)\n",
    "\n",
    "client.indices.create(\n",
    "    index=index_name,\n",
    "    settings=configurations[\"settings\"],\n",
    "    mappings=configurations[\"mappings\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd90bde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████▉| 1207501/1209001 [04:36<00:00, 4717.47docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1208362/1209001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|███████████████████████████████████████████████████████████████████▉| 1208362/1209001 [04:50<00:00, 4717.47docs/s]"
     ]
    }
   ],
   "source": [
    "print(\"Indexing documents...\")\n",
    "number_of_docs=1209001\n",
    "progress = tqdm(unit=\"docs\", total=number_of_docs)\n",
    "successes = 0\n",
    "for ok, action in streaming_bulk(\n",
    "    client=client, index=index_name, actions=generate_actions(),\n",
    "):\n",
    "    progress.update(1)\n",
    "    successes += ok\n",
    "print(\"Indexed %d/%d documents\" % (successes, number_of_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54444219",
   "metadata": {},
   "source": [
    "# Answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f2af0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(question, index_name=\"offline_competition\", k=3):\n",
    "    resp = client.search(index=index_name, \n",
    "                     query={'match': {\n",
    "                         \"article\": question\n",
    "                     }})\n",
    "    best_documents = list(sorted(resp['hits']['hits'], key=lambda k: k['_score'], reverse=True))\n",
    "    context = \"\"\n",
    "    for document in best_documents[:k]:\n",
    "        if document['_source']['title'].lower() not in document['_source']['article'].lower():\n",
    "            context += document['_source']['title'] + \" . \"\n",
    "        context += document['_source']['article'] + \"\\n\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a33e90",
   "metadata": {},
   "source": [
    "## Herbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6aa0322",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29703d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'HerbertAnswers/test_B_offline_competition_k_{k}.txt', 'a', encoding='UTF-8') as f:\n",
    "    with open(f'HerbertAnswers/test_B_offline_competition_k_{k}_with_confidence.txt', 'a', encoding='UTF-8') as f_confidence:\n",
    "        for question, _ in tqdm(question_answers):\n",
    "            context = retriever(question, index_name=\"offline_competition\", k=k)\n",
    "            result = get_answer_herbert(context, question)\n",
    "            predicted_answer = result['answer'].replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "            confidence = result['score'] if predicted_answer != \"\" else 0.0\n",
    "            f.write(predicted_answer + \"\\n\")\n",
    "            f_confidence.write(f\"{predicted_answer}\\t{confidence}\\n\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f77770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'HerbertAnswers/test_B_offline_competition_k_{k}.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('found_answers.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3df89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('correct_answers.txt', 'w', encoding='UTF-8') as f:\n",
    "    for _, answers in question_answers:\n",
    "        f.write(\"\\t\".join(answers) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a409fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 0.2384\n"
     ]
    }
   ],
   "source": [
    "!python advent_answer_check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56823c23",
   "metadata": {},
   "source": [
    "## PLT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "528c9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6213211d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2500/2500 [1:32:33<00:00,  2.22s/it]\n"
     ]
    }
   ],
   "source": [
    "with open(f'PLT5BaseAnswers/test_B_offline_competition_k_{k}.txt', 'a', encoding='UTF-8') as f:\n",
    "    with open(f'PLT5BaseAnswers/test_B_offline_competition_k_{k}_with_confidence.txt', 'a', encoding='UTF-8') as f_confidence:\n",
    "        for question, _ in tqdm(question_answers):\n",
    "            context = retriever(question, index_name=\"offline_competition\", k=k)\n",
    "            result = get_answer_plt5(context, question)\n",
    "            predicted_answer = result['answer'].replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "            confidence = result['score'] if predicted_answer != \"\" else 0.0\n",
    "            f.write(predicted_answer + \"\\n\")\n",
    "            f_confidence.write(f\"{predicted_answer}\\t{confidence}\\n\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c80c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'PLT5BaseAnswers/test_B_offline_competition_k_{k}.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('found_answers.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1218a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('found_answers.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('FinalAnswers/TestB_Offline.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f286526f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('correct_answers.txt', 'w', encoding='UTF-8') as f:\n",
    "    for _, answers in question_answers:\n",
    "        f.write(\"\\t\".join(answers) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5536c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 0.3744\n"
     ]
    }
   ],
   "source": [
    "!python advent_answer_check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1774c58",
   "metadata": {},
   "source": [
    "# Join Herbert and PLT5 answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c35efa",
   "metadata": {},
   "source": [
    "## Herbert > PLT5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479944f",
   "metadata": {},
   "source": [
    "Worse than pure PLT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "056f890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'PLT5BaseAnswers/test_B_offline_competition_k_5.txt', 'r', encoding='UTF-8') as f_plt5:\n",
    "    with open(f'HerbertAnswers/test_B_offline_competition_k_3.txt', 'r', encoding='UTF-8') as f_herbert:\n",
    "        with open(f'HerbertAndPLT5Answers/test_B_offline_competition_answer_with_PLT5_if_no_Herbert_answer.txt', 'a',\n",
    "                  encoding='UTF-8') as f_combined:\n",
    "            for herbert_answer, plt5_answer in zip(f_herbert, f_plt5):\n",
    "                herbert_answer = herbert_answer.strip()\n",
    "                plt5_answer = plt5_answer.strip()\n",
    "                if herbert_answer != \"\":\n",
    "                    f_combined.write(herbert_answer + \"\\n\")\n",
    "                else:\n",
    "                    f_combined.write(plt5_answer + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94d2f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'HerbertAndPLT5Answers/test_B_offline_competition_answer_with_PLT5_if_no_Herbert_answer.txt', 'r',\n",
    "          encoding='UTF-8') as f:\n",
    "    with open('found_answers.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67dff771",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('correct_answers.txt', 'w', encoding='UTF-8') as f:\n",
    "    for _, answers in question_answers:\n",
    "        f.write(\"\\t\".join(answers) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fa9922e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 0.3372\n"
     ]
    }
   ],
   "source": [
    "!python advent_answer_check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97827eed",
   "metadata": {},
   "source": [
    "## PLT5 > Herbert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe4b276",
   "metadata": {},
   "source": [
    "Better than pure PLT5 and Herbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0130c668",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'PLT5BaseAnswers/test_B_offline_competition_k_5.txt', 'r', encoding='UTF-8') as f_plt5:\n",
    "    with open(f'HerbertAnswers/test_B_offline_competition_k_3.txt', 'r', encoding='UTF-8') as f_herbert:\n",
    "        with open(f'HerbertAndPLT5Answers/test_B_offline_competition_answer_with_Herbert_if_no_PLT5_answer.txt', 'a',\n",
    "                  encoding='UTF-8') as f_combined:\n",
    "            for herbert_answer, plt5_answer in zip(f_herbert, f_plt5):\n",
    "                herbert_answer = herbert_answer.strip()\n",
    "                plt5_answer = plt5_answer.strip()\n",
    "                if plt5_answer != \"\":\n",
    "                    f_combined.write(plt5_answer + \"\\n\")\n",
    "                else:\n",
    "                    f_combined.write(herbert_answer + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d5900bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'HerbertAndPLT5Answers/test_B_offline_competition_answer_with_Herbert_if_no_PLT5_answer.txt', 'r',\n",
    "          encoding='UTF-8') as f:\n",
    "    with open('found_answers.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29c8879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('found_answers.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('FinalAnswers/TestB_Offline.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75c8fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('correct_answers.txt', 'w', encoding='UTF-8') as f:\n",
    "    for _, answers in question_answers:\n",
    "        f.write(\"\\t\".join(answers) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77a7666a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 0.3764\n"
     ]
    }
   ],
   "source": [
    "!python advent_answer_check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90ac01",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2413f",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0cee1",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b6b070",
   "metadata": {},
   "source": [
    "------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28623821",
   "metadata": {},
   "source": [
    "#  Modifications below didn't improve score of combined PLT5 and Herbert  answers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20596820",
   "metadata": {},
   "source": [
    "# Handle special type of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c4cbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [question for question, _ in question_answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a1358",
   "metadata": {},
   "source": [
    "## Yes/No questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98aac1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_yes_no_question(question):\n",
    "    return question[:4] == \"Czy \" and \" czy \" not in question[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db46afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_prob(text):\n",
    "    input_ids = torch.tensor(yes_no_tokenizer.encode(text)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = yes_no_model(input_ids, labels=input_ids)\n",
    "    loss, logits = outputs[:2]\n",
    "    sentence_prob = loss.item()\n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6b91d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_yes_no_question(question):\n",
    "    yes_sentence_prob = get_sentence_prob(question + \" Tak\")\n",
    "    no_sentence_prob = get_sentence_prob(question + \" Nie\")\n",
    "    return \"tak\" if yes_sentence_prob > no_sentence_prob else \"nie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c79534af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_yes_no_answer(answer):\n",
    "    answer = answer.strip().lower()\n",
    "    return answer == \"tak\" or answer == \"nie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d931f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'HerbertAndPLT5Answers/test_B_offline_competition_answer_with_Herbert_if_no_PLT5_answer.txt', 'r',\n",
    "          encoding='UTF-8') as f:\n",
    "    with open(f'HerbertAndPLT5Answers/test_B_offline_competition_answer_with_Herbert_if_no_PLT5_answer_yes_no.txt', 'w',\n",
    "              encoding='UTF-8') as f_answers:\n",
    "        raw_answers = []\n",
    "        for line in f:\n",
    "            raw_answers.append(line.strip())\n",
    "        for question, raw_answer in zip(questions, raw_answers):\n",
    "            if is_yes_no_question(question) and not is_yes_no_answer(raw_answer):\n",
    "                f_answers.write(answer_yes_no_question(question) + \"\\n\")\n",
    "            else:\n",
    "                f_answers.write(raw_answer + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a999bd3",
   "metadata": {},
   "source": [
    "### Test B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61fc5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HerbertAndPLT5Answers/test_B_offline_competition_answer_with_Herbert_if_no_PLT5_answer_yes_no.txt', 'r',\n",
    "          encoding='UTF-8') as f:\n",
    "    with open('found_answers.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1139a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('found_answers.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('FinalAnswers/TestB_Offline.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "359fd9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('correct_answers.txt', 'w', encoding='UTF-8') as f:\n",
    "    for _, answers in question_answers:\n",
    "        f.write(\"\\t\".join(answers) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82554b6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 0.3764\n"
     ]
    }
   ],
   "source": [
    "!python advent_answer_check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b4449",
   "metadata": {},
   "source": [
    "## Optional questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1732dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_optional_question(question):\n",
    "    to_pointer = question.find(\" to \")\n",
    "    if to_pointer == -1:\n",
    "        return False\n",
    "    czy_part = question[to_pointer:]\n",
    "    czy_pointer = czy_part.find(\" czy \")\n",
    "    return czy_pointer != -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c06bb097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_and_options(question):\n",
    "    if (dot_pointer := question.find(\".\")) != -1:\n",
    "        question = question[dot_pointer:]\n",
    "    if question[-1] == \"?\":\n",
    "        question = question[:-1]\n",
    "    if question.lower()[:3] == 'czy':\n",
    "        question = question[3:].strip()\n",
    "    term, options = question.split(' to ')\n",
    "    options = options.split(' czy ')\n",
    "    first_options = options[0]\n",
    "    first_options = first_options.split(\",\")\n",
    "    first_options.extend(options[1:])\n",
    "    return term.strip(), [opt.strip() for opt in first_options if opt.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "343d9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_optional_question(question):\n",
    "    term, options = get_term_and_options(question)\n",
    "    options_embeddings = embedding_model.encode(options)\n",
    "    term_embedding = embedding_model.encode(term)\n",
    "    similarity = np.array([distance.cosine(term_embedding, emb) for emb in options_embeddings])\n",
    "    return options[np.argmin(similarity)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03353bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'HerbertAndPLT5Answers/test_B_offline_competition_answer_with_Herbert_if_no_PLT5_answer_yes_no.txt', 'r',\n",
    "          encoding='UTF-8') as f:\n",
    "    with open('HerbertAndPLT5Answers/test_B_offline_competition_answer_with_Herbert_if_no_PLT5_answer_yes_no_optional.txt', 'w',\n",
    "              encoding='UTF-8') as f_answers:\n",
    "        raw_answers = []\n",
    "        for line in f:\n",
    "            raw_answers.append(line.strip())\n",
    "        for question, raw_answer in zip(questions, raw_answers):\n",
    "            if not is_yes_no_question(question) and is_optional_question(question):\n",
    "                f_answers.write(answer_optional_question(question) + \"\\n\")\n",
    "            else:\n",
    "                f_answers.write(raw_answer + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd361f0",
   "metadata": {},
   "source": [
    "### Test B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73b89cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HerbertAndPLT5Answers/test_B_offline_competition_answer_with_Herbert_if_no_PLT5_answer_yes_no_optional.txt', 'r',\\\n",
    "          encoding='UTF-8') as f:\n",
    "    with open('found_answers.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c5011bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('found_answers.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('FinalAnswers/TestB_Offline.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c349b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('correct_answers.txt', 'w', encoding='UTF-8') as f:\n",
    "    for _, answers in question_answers:\n",
    "        f.write(\"\\t\".join(answers) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "69efde8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 0.3732\n"
     ]
    }
   ],
   "source": [
    "!python advent_answer_check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63406fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
