{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f907e344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from elasticsearch.helpers import streaming_bulk\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModel\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62f505",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60375a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/test_B_questions.txt', 'r', encoding='UTF-8') as f:\n",
    "    questions = []\n",
    "    for line in f:\n",
    "        questions.append(line.strip())\n",
    "        \n",
    "with open('../Data/test_B_answers.txt', 'r', encoding='UTF-8') as f:\n",
    "    answers = []\n",
    "    for line in f:\n",
    "        splitted = line.strip().split(\"\\t\")\n",
    "        answers.append(splitted)\n",
    "\n",
    "question_answers = list(zip(questions, answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ef0959b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Proszę podać datę zamachu na WTC.',\n",
       " 'Czy interna to szpitalny oddział chorób zakaźnych',\n",
       " 'Proszę podać nazwiska skłóconych rodów, z których pochodzili Romeo i Julia.',\n",
       " 'Dokończ powiedzenie: „Ten się śmieje, kto...\"',\n",
       " 'Proszę podać rok wprowadzenia w Polsce stanu wojennego.',\n",
       " 'Z którego kontynentu pochodzi język suahili',\n",
       " 'Dokończ przysłowie: „i wilk syty, i ...”',\n",
       " 'Podaj polski tytuł filmu z serii o Jamesie Bondzie, w którym wystąpiła Madonna.',\n",
       " 'Podaj dwa imiona Roosevelta, 32. prezydenta Stanów Zjednoczonych.',\n",
       " 'Który przypadek odpowiada na pytania: „o kim? o czym?”',\n",
       " 'Proszę podać oba imiona Bacha.',\n",
       " 'Proszę podać nazwiska dwóch amerykańskich aktorów, którzy zagrali główne męskie role w filmie „Żądło”.',\n",
       " 'Wielka i Mała Rawka to szczyty Bieszczadów czy Gór Świętokrzyskich',\n",
       " 'W którym mieście urodził się Mahomet',\n",
       " 'Proszę dokończyć przysłowie: „Przyganiał kocioł garnkowi...”',\n",
       " 'Warwick to największy średniowieczny zamek na Wyspach Brytyjskich leżący w Anglii, Szkocji czy Irlandii Północnej',\n",
       " 'Proszę podać symbol chemiczny tlenu.',\n",
       " 'Dokończ przysłowie: \"Kiedy wejdziesz między wrony, musisz krakać jak...\"',\n",
       " 'Jak nazywa się zewnętrzna warstwa kuli Ziemskiej, obejmująca skorupę i górną warstwę płaszcza? Słowo pochodzi z greki.',\n",
       " 'Proszę dokończyć przysłowie: „Kto pod kim dołki kopie...”',\n",
       " 'Proszę podać imię i nazwisko pierwszego polskiego mistrza olimpijskiego w boksie.',\n",
       " 'Proszę podać polską nazwę pierwiastka natrium.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[question for question, answers in question_answers if \"?\" != question[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc5f65",
   "metadata": {},
   "source": [
    "# Pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f88c5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_no_model = AutoModelWithLMHead.from_pretrained('flax-community/papuGaPT2')\n",
    "yes_no_tokenizer = AutoTokenizer.from_pretrained('flax-community/papuGaPT2')\n",
    "# yes_no_tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-large-cased\")\n",
    "# yes_no_model = AutoModelWithLMHead.from_pretrained(\"allegro/herbert-large-cased\")\n",
    "_ = yes_no_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca6322",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b265af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model='azwierzc/herbert-large-poquad', handle_impossible_answer=True,\n",
    "    tokenizer='azwierzc/herbert-large-poquad'\n",
    ")\n",
    "\n",
    "def get_answer(context, question):\n",
    "    return qa_pipeline({\n",
    "        'context': context,\n",
    "        'question': question})['answer'].replace(\"\\n\", \" \").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ee40b",
   "metadata": {},
   "source": [
    "# Connect to elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fb961616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Password for the 'elastic' user generated by Elasticsearch\n",
    "ELASTIC_PASSWORD = FILL_IN_PASSWORD\n",
    "# es_path = \"~/Documents/UWr/Chatbots/elasticsearch-8.4.3/\"\n",
    "es_path = \"C:/Users/jakub/elasticsearch-8.5.3-windows-x86_64/elasticsearch-8.5.3/\"\n",
    "\n",
    "# Create the client instance\n",
    "client = Elasticsearch(\n",
    "    \"https://localhost:9200\",\n",
    "    ca_certs=es_path+\"config/certs/http_ca.crt\",\n",
    "    basic_auth=(\"elastic\", ELASTIC_PASSWORD)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b715b014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if you can connect to ES (bool)\n",
    "client.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e81d04",
   "metadata": {},
   "source": [
    "# Index documents from wikipedia paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48f87869",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILE = \"../Data/fp_wiki.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20d37483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_actions():\n",
    "    with open(DATASET_FILE, \"r\", encoding=\"UTF-8\") as file:\n",
    "        # Read the first line of the file\n",
    "        title_line = file.readline()\n",
    "        # Create a variable to store the ID of the next document\n",
    "        next_id = 0\n",
    "        # Keep reading lines until the end of the file is reached\n",
    "        while title_line:\n",
    "            # Check if the line starts with \"Title: \"\n",
    "            if title_line.startswith(\"TITLE: \"):\n",
    "                # Get the title by stripping the \"Title: \" prefix and the newline character at the end\n",
    "                title = title_line.lstrip(\"TITLE:\").strip()\n",
    "                # Read the second line of the file, which should be the title again\n",
    "                title_line = file.readline().strip()\n",
    "                # Save title for later usage\n",
    "                title = title_line\n",
    "                # Create a list to store the lines of the article\n",
    "                article_lines = []\n",
    "                # Read the next line, which should be the start of the article\n",
    "                article_line = file.readline()\n",
    "                # Keep reading lines until an empty line is reached\n",
    "                while article_line.strip():\n",
    "                    # Add the line to the list of article lines\n",
    "                    article_lines.append(article_line)\n",
    "                    # Read the next line\n",
    "                    article_line = file.readline()\n",
    "                # Join the lines of the article with newline characters to create the article\n",
    "                article = \"\\n\".join(article_lines) if article_lines else \"\"\n",
    "                # Create a dictionary for the document\n",
    "                document = {\"_id\": next_id, \"title\": title, \"article\": article}\n",
    "                # Yield new document\n",
    "                yield document\n",
    "                # Increment the ID for the next document\n",
    "                next_id += 1\n",
    "                # Read the next line, which should be the start of the next document\n",
    "                title_line = file.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9c5eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"offline_competition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84f2ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "configurations = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"lang_pl_morfologik\": { \n",
    "                    \"type\": \"custom\",\n",
    "                    \"tokenizer\": \"standard\",\n",
    "                    \"filter\": [\n",
    "                        \"morfologik_stem\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"id\": {\"type\": \"long\"},\n",
    "            \"article\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"lang_pl_morfologik\"\n",
    "            },\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"lang_pl_morfologik\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff863ced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'offline_competition'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.options(ignore_status=[400,404]).indices.delete(index=index_name)\n",
    "\n",
    "client.indices.create(\n",
    "    index=index_name,\n",
    "    settings=configurations[\"settings\"],\n",
    "    mappings=configurations[\"mappings\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd90bde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████▉| 1207501/1209001 [04:36<00:00, 4717.47docs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1208362/1209001 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|███████████████████████████████████████████████████████████████████▉| 1208362/1209001 [04:50<00:00, 4717.47docs/s]"
     ]
    }
   ],
   "source": [
    "print(\"Indexing documents...\")\n",
    "number_of_docs=1209001\n",
    "progress = tqdm(unit=\"docs\", total=number_of_docs)\n",
    "successes = 0\n",
    "for ok, action in streaming_bulk(\n",
    "    client=client, index=index_name, actions=generate_actions(),\n",
    "):\n",
    "    progress.update(1)\n",
    "    successes += ok\n",
    "print(\"Indexed %d/%d documents\" % (successes, number_of_docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54444219",
   "metadata": {},
   "source": [
    "# Answer questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f2af0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retriever(question, index_name=\"offline_competition\", k=3):\n",
    "    resp = client.search(index=index_name, \n",
    "                     query={'match': {\n",
    "                         \"article\": question\n",
    "                     }})\n",
    "    best_documents = list(sorted(resp['hits']['hits'], key=lambda k: k['_score'], reverse=True))\n",
    "    context = \"\"\n",
    "    for document in best_documents[:k]:\n",
    "        if document['_source']['title'].lower() not in document['_source']['article'].lower():\n",
    "            context += document['_source']['title'] + \" . \"\n",
    "        context += document['_source']['article'] + \"\\n\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27a44ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2500/2500 [39:38<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('HerbertAnswers/test_B_offline_competition_k_3.txt', 'a', encoding='UTF-8') as f:\n",
    "    for question, _ in tqdm(question_answers):\n",
    "        context = retriever(question, index_name=\"offline_competition\", k=3)\n",
    "        predicted_answer = get_answer(context, question)\n",
    "        f.write(predicted_answer.replace(\"\\n\", \" \").strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f77770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HerbertAnswers/test_B_offline_competition_k_3.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('found_answers.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3df89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('correct_answers.txt', 'w', encoding='UTF-8') as f:\n",
    "    for _, answers in question_answers:\n",
    "        f.write(\"\\t\".join(answers) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a409fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 0.2384\n"
     ]
    }
   ],
   "source": [
    "!python advent_answer_check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20596820",
   "metadata": {},
   "source": [
    "# Handle special type of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9c4cbbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [question for question, _ in question_answers]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a1358",
   "metadata": {},
   "source": [
    "## Yes/No questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98aac1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_yes_no_question(question):\n",
    "    return question[:4] == \"Czy \" and \" czy \" not in question[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db46afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_prob(text):\n",
    "    input_ids = torch.tensor(yes_no_tokenizer.encode(text)).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = yes_no_model(input_ids, labels=input_ids)\n",
    "    loss, logits = outputs[:2]\n",
    "    sentence_prob = loss.item()\n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e6b91d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_yes_no_question(question):\n",
    "    yes_sentence_prob = get_sentence_prob(question + \" Tak\")\n",
    "    no_sentence_prob = get_sentence_prob(question + \" Nie\")\n",
    "    return \"tak\" if yes_sentence_prob > no_sentence_prob else \"nie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d931f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HerbertAnswers/test_B_offline_competition_k_3.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('HerbertAnswers/test_B_offline_competition_k_3_yes_no.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        raw_answers = []\n",
    "        for line in f:\n",
    "            raw_answers.append(line.strip())\n",
    "        for question, raw_answer in zip(questions, raw_answers):\n",
    "            if is_yes_no_question(question):\n",
    "                f_answers.write(answer_yes_no_question(question) + \"\\n\")\n",
    "            else:\n",
    "                f_answers.write(raw_answer + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a999bd3",
   "metadata": {},
   "source": [
    "### Test B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "61fc5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HerbertAnswers/test_B_offline_competition_k_3_yes_no.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('found_answers.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1139a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('found_answers.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('FinalAnswers/TestB_Offline.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "359fd9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('correct_answers.txt', 'w', encoding='UTF-8') as f:\n",
    "    for _, answers in question_answers:\n",
    "        f.write(\"\\t\".join(answers) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82554b6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 0.2764\n"
     ]
    }
   ],
   "source": [
    "!python advent_answer_check.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5b4449",
   "metadata": {},
   "source": [
    "## Optional questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1732dc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_optional_question(question):\n",
    "    to_pointer = question.find(\" to \")\n",
    "    if to_pointer == -1:\n",
    "        return False\n",
    "    czy_part = question[to_pointer:]\n",
    "    czy_pointer = czy_part.find(\" czy \")\n",
    "    return czy_pointer != -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c06bb097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_and_options(question):\n",
    "    if (dot_pointer := question.find(\".\")) != -1:\n",
    "        question = question[dot_pointer:]\n",
    "    if question[-1] == \"?\":\n",
    "        question = question[:-1]\n",
    "    if question.lower()[:3] == 'czy':\n",
    "        question = question[3:].strip()\n",
    "    term, options = question.split(' to ')\n",
    "    options = options.split(' czy ')\n",
    "    first_options = options[0]\n",
    "    first_options = first_options.split(\",\")\n",
    "    first_options.extend(options[1:])\n",
    "    return term.strip(), [opt.strip() for opt in first_options if opt.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "343d9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_optional_question(question):\n",
    "    term, options = get_term_and_options(question)\n",
    "    options_embeddings = embedding_model.encode(options)\n",
    "    term_embedding = embedding_model.encode(term)\n",
    "    similarity = np.array([distance.cosine(term_embedding, emb) for emb in options_embeddings])\n",
    "    return options[np.argmin(similarity)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "03353bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HerbertAnswers/test_B_offline_competition_k_3_yes_no.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('HerbertAnswers/test_B_offline_competition_k_3_yes_no_optional.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        raw_answers = []\n",
    "        for line in f:\n",
    "            raw_answers.append(line.strip())\n",
    "        for question, raw_answer in zip(questions, raw_answers):\n",
    "            if not is_yes_no_question(question) and is_optional_question(question):\n",
    "                f_answers.write(answer_optional_question(question) + \"\\n\")\n",
    "            else:\n",
    "                f_answers.write(raw_answer + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd361f0",
   "metadata": {},
   "source": [
    "### Test B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "73b89cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HerbertAnswers/test_B_offline_competition_k_3_yes_no_optional.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('found_answers.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c5011bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('found_answers.txt', 'r', encoding='UTF-8') as f:\n",
    "    with open('FinalAnswers/TestB_Offline.txt', 'w', encoding='UTF-8') as f_answers:\n",
    "        for line in f:\n",
    "            f_answers.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c349b428",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('correct_answers.txt', 'w', encoding='UTF-8') as f:\n",
    "    for _, answers in question_answers:\n",
    "        f.write(\"\\t\".join(answers) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "69efde8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOTAL SCORE: 0.2884\n"
     ]
    }
   ],
   "source": [
    "!python advent_answer_check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63406fab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
